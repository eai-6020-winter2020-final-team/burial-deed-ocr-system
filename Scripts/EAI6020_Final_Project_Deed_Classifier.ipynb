{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for Deed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Packages requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opencv-python        4.1.2.30   \n",
    "six                  1.13.0             \n",
    "slim                 0.1                \n",
    "tensorboard          2.0.2              \n",
    "tensorflow           2.0.0              \n",
    "tensorflow-estimator 2.0.1              \n",
    "tensorflow-gpu       2.0.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH='~/models-master/research/slim'\n"
     ]
    }
   ],
   "source": [
    "#set PATH=C\n",
    "#echo %PATH% \n",
    "\n",
    "# here is NETS package pwd\n",
    "#PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "%env PYTHONPATH =  '~/models-master/research/slim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_old(img,name):\n",
    "    cv2.namedWindow(name, cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(name, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def img_show(img, code=cv2.COLOR_BGR2RGB):\n",
    "    cv_rgb = cv2.cvtColor(img, code)\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.imshow(cv_rgb)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_arr(img,x,y):\n",
    "    img = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)).convert('L')\n",
    "    #img.show()  \n",
    "    #img = Image.open('iCard_021873_1_Daba_Ayehush_H-Copy1.jpg').convert('L')\n",
    "    if img.size[0] != x or img.size[1] != y:\n",
    "        img = img.resize((x, y))\n",
    "\n",
    "    arr = []\n",
    "\n",
    "    for i in range(y):\n",
    "        for j in range(x):\n",
    "            # mnist 里的颜色是0代表白色（背景），1.0代表黑色\n",
    "            #print(img.getpixel((j, i)))\n",
    "            pixel = 1.0 - float(img.getpixel((j, i)))/255.0\n",
    "            # pixel = 255.0 - float(img.getpixel((j, i))) # 如果是0-255的颜色值\n",
    "            arr.append(pixel)\n",
    "            \n",
    "    return arr\n",
    "\n",
    "#img_to_arr(img_list[0],300,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 90000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = sorted(os.listdir('All_Data/'))\n",
    "#Returns a list of all folders with participant numbers\n",
    "img_list =[]\n",
    "for path in img_path:\n",
    "    img  = cv2.imread('All_Data/' + path) \n",
    "    img_list.append(img)\n",
    "    \n",
    "text_list = []    \n",
    "for img in img_list:\n",
    "    text_list.append(img_to_arr(img,300,300))\n",
    "    \n",
    "np.array(text_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data = pd.read_csv('label_data.csv')\n",
    "\n",
    "img_group_list = []\n",
    "\n",
    "img_group = []\n",
    "for i in np.array(img_data.iloc[:,[1]]).tolist():\n",
    "    if(i == [\"N\"]):\n",
    "        img_group.append([0])\n",
    "    elif(i == [\"Y\"]):\n",
    "        img_group.append([1])\n",
    "img_group_list.append(img_group)  \n",
    "\n",
    "img_group = []\n",
    "for i in np.array(img_data.iloc[:,[2]]).tolist():\n",
    "    if(i == [\"N\"]):\n",
    "        img_group.append([0])\n",
    "    elif(i == [\"Y\"]):\n",
    "        img_group.append([1])\n",
    "img_group_list.append(img_group)   \n",
    "\n",
    "\n",
    "len(img_group_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 300, 300, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train = np.array(text_list)\n",
    "input_train = input_train.reshape(input_train.shape[0], 300, 300, 1)\n",
    "input_train.shape\n",
    "\n",
    "input_train[0:450].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Keras CNN\n",
    "'''\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "# Model configuration\n",
    "img_width, img_height = 500, 500\n",
    "batch_size = 250\n",
    "no_epochs = 100\n",
    "no_classes = 3\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "def kares_data(input_data,input_target,img_width,img_height,no_classes):\n",
    "        # Load dataset\n",
    "    input_data = np.array(input_data)\n",
    "    input_train = input_data[0:450]\n",
    "    target_train = input_target[0:450]\n",
    "    \n",
    "    input_test = input_data[450:520]\n",
    "    target_test = input_target[450:520]\n",
    "    \n",
    "    # Reshape data based on channels first / channels last strategy.\n",
    "    # This is dependent on whether you use TF, Theano or CNTK as backend.\n",
    "    # Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_train = input_train.reshape(input_train.shape[0], 1, img_width, img_height)\n",
    "        input_test = input_test.reshape(input_test.shape[0], 1, img_width, img_height)\n",
    "        input_shape = (1, img_width, img_height)\n",
    "    else:\n",
    "        input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\n",
    "        input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\n",
    "        input_shape = (img_width, img_height, 1)\n",
    "    \n",
    "    # Parse numbers as floats\n",
    "    input_train = input_train.astype('float32')\n",
    "    input_test = input_test.astype('float32')\n",
    "    \n",
    "    # Normalize data\n",
    "    input_train = input_train / 255\n",
    "    input_test = input_test / 255\n",
    "    \n",
    "    # Convert target vectors to categorical targets\n",
    "    target_train = keras.utils.to_categorical(target_train, no_classes)\n",
    "    target_test = keras.utils.to_categorical(target_test, no_classes)\n",
    "    \n",
    "    return input_train,input_test,target_train,target_test,input_shape\n",
    "    \n",
    "def kares_setup(input_train,input_test,target_train,target_test,input_shape,img_width, img_height,batch_size,no_epochs,no_classes,validation_split,verbosity):\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(no_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Fit data to model\n",
    "    model.fit(input_train, target_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=no_epochs,\n",
    "          verbose=verbosity,\n",
    "          validation_split=validation_split)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    score = model.evaluate(input_test, target_test, verbose=0)\n",
    "    print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360 samples, validate on 90 samples\n",
      "Epoch 1/50\n",
      "360/360 [==============================] - 10s 28ms/sample - loss: 0.6878 - accuracy: 0.6222 - val_loss: 0.7570 - val_accuracy: 0.7333\n",
      "Epoch 2/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.8710 - accuracy: 0.6278 - val_loss: 0.6924 - val_accuracy: 0.7333\n",
      "Epoch 3/50\n",
      "360/360 [==============================] - 8s 22ms/sample - loss: 0.6928 - accuracy: 0.6278 - val_loss: 0.6915 - val_accuracy: 0.7333\n",
      "Epoch 4/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6917 - accuracy: 0.6278 - val_loss: 0.6887 - val_accuracy: 0.7333\n",
      "Epoch 5/50\n",
      "360/360 [==============================] - 8s 22ms/sample - loss: 0.6906 - accuracy: 0.6278 - val_loss: 0.6858 - val_accuracy: 0.7333\n",
      "Epoch 6/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6889 - accuracy: 0.6278 - val_loss: 0.6819 - val_accuracy: 0.7333\n",
      "Epoch 7/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6868 - accuracy: 0.6278 - val_loss: 0.6774 - val_accuracy: 0.7333\n",
      "Epoch 8/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6844 - accuracy: 0.6278 - val_loss: 0.6725 - val_accuracy: 0.7333\n",
      "Epoch 9/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6821 - accuracy: 0.6278 - val_loss: 0.6675 - val_accuracy: 0.7333\n",
      "Epoch 10/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6796 - accuracy: 0.6278 - val_loss: 0.6624 - val_accuracy: 0.7333\n",
      "Epoch 11/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.6772 - accuracy: 0.6278 - val_loss: 0.6575 - val_accuracy: 0.7333\n",
      "Epoch 12/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6748 - accuracy: 0.6278 - val_loss: 0.6527 - val_accuracy: 0.7333\n",
      "Epoch 13/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6726 - accuracy: 0.6278 - val_loss: 0.6479 - val_accuracy: 0.7333\n",
      "Epoch 14/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6706 - accuracy: 0.6278 - val_loss: 0.6433 - val_accuracy: 0.7333\n",
      "Epoch 15/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6691 - accuracy: 0.6278 - val_loss: 0.6388 - val_accuracy: 0.7333\n",
      "Epoch 16/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6671 - accuracy: 0.6278 - val_loss: 0.6346 - val_accuracy: 0.7333\n",
      "Epoch 17/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6660 - accuracy: 0.6278 - val_loss: 0.6306 - val_accuracy: 0.7333\n",
      "Epoch 18/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6646 - accuracy: 0.6278 - val_loss: 0.6270 - val_accuracy: 0.7333\n",
      "Epoch 19/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6635 - accuracy: 0.6278 - val_loss: 0.6236 - val_accuracy: 0.7333\n",
      "Epoch 20/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6628 - accuracy: 0.6278 - val_loss: 0.6204 - val_accuracy: 0.7333\n",
      "Epoch 21/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6619 - accuracy: 0.6278 - val_loss: 0.6176 - val_accuracy: 0.7333\n",
      "Epoch 22/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6616 - accuracy: 0.6278 - val_loss: 0.6151 - val_accuracy: 0.7333\n",
      "Epoch 23/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6609 - accuracy: 0.6278 - val_loss: 0.6131 - val_accuracy: 0.7333\n",
      "Epoch 24/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6606 - accuracy: 0.6278 - val_loss: 0.6112 - val_accuracy: 0.7333\n",
      "Epoch 25/50\n",
      "360/360 [==============================] - 8s 24ms/sample - loss: 0.6603 - accuracy: 0.6278 - val_loss: 0.6095 - val_accuracy: 0.7333\n",
      "Epoch 26/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6078 - val_accuracy: 0.7333\n",
      "Epoch 27/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6063 - val_accuracy: 0.7333\n",
      "Epoch 28/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6050 - val_accuracy: 0.7333\n",
      "Epoch 29/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6040 - val_accuracy: 0.7333\n",
      "Epoch 30/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6603 - accuracy: 0.6278 - val_loss: 0.6033 - val_accuracy: 0.7333\n",
      "Epoch 31/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6030 - val_accuracy: 0.7333\n",
      "Epoch 32/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6029 - val_accuracy: 0.7333\n",
      "Epoch 33/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6028 - val_accuracy: 0.7333\n",
      "Epoch 34/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6027 - val_accuracy: 0.7333\n",
      "Epoch 35/50\n",
      "360/360 [==============================] - 8s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6025 - val_accuracy: 0.7333\n",
      "Epoch 36/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6025 - val_accuracy: 0.7333\n",
      "Epoch 37/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6024 - val_accuracy: 0.7333\n",
      "Epoch 38/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6025 - val_accuracy: 0.7333\n",
      "Epoch 39/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6029 - val_accuracy: 0.7333\n",
      "Epoch 40/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6034 - val_accuracy: 0.7333\n",
      "Epoch 41/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6037 - val_accuracy: 0.7333\n",
      "Epoch 42/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6041 - val_accuracy: 0.7333\n",
      "Epoch 43/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6042 - val_accuracy: 0.7333\n",
      "Epoch 44/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6044 - val_accuracy: 0.7333\n",
      "Epoch 45/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6045 - val_accuracy: 0.7333\n",
      "Epoch 46/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6046 - val_accuracy: 0.7333\n",
      "Epoch 47/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6048 - val_accuracy: 0.7333\n",
      "Epoch 48/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6049 - val_accuracy: 0.7333\n",
      "Epoch 49/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6602 - accuracy: 0.6278 - val_loss: 0.6052 - val_accuracy: 0.7333\n",
      "Epoch 50/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.6601 - accuracy: 0.6278 - val_loss: 0.6054 - val_accuracy: 0.7333\n",
      "Test loss: 0.5114448274884905 / Test accuracy: 0.9142857193946838\n"
     ]
    }
   ],
   "source": [
    "# Hand writing classifier\n",
    "keras.backend.clear_session()\n",
    "\n",
    "input_train,input_test,target_train,target_test,input_shape = kares_data(text_list,img_group_list[0],300,300,2)\n",
    "\n",
    "model_handwriting = kares_setup(input_train,input_test,target_train,target_test,input_shape, 300, 300, 250, 50, 2 , 0.2 ,1)\n",
    "\n",
    "model_handwriting.save('model_hw_1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360 samples, validate on 90 samples\n",
      "Epoch 1/50\n",
      "360/360 [==============================] - 9s 26ms/sample - loss: 0.6927 - accuracy: 0.4917 - val_loss: 0.6343 - val_accuracy: 0.6889\n",
      "Epoch 2/50\n",
      "360/360 [==============================] - 9s 26ms/sample - loss: 0.6884 - accuracy: 0.5333 - val_loss: 0.7510 - val_accuracy: 0.3111\n",
      "Epoch 3/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.7010 - accuracy: 0.4722 - val_loss: 0.6698 - val_accuracy: 0.7111\n",
      "Epoch 4/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6811 - accuracy: 0.5972 - val_loss: 0.7109 - val_accuracy: 0.3444\n",
      "Epoch 5/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6821 - accuracy: 0.5111 - val_loss: 0.6834 - val_accuracy: 0.6444\n",
      "Epoch 6/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6603 - accuracy: 0.6278 - val_loss: 0.6059 - val_accuracy: 0.7222\n",
      "Epoch 7/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6526 - accuracy: 0.6278 - val_loss: 0.6334 - val_accuracy: 0.6778\n",
      "Epoch 8/50\n",
      "360/360 [==============================] - 8s 24ms/sample - loss: 0.6373 - accuracy: 0.6333 - val_loss: 0.5996 - val_accuracy: 0.7111\n",
      "Epoch 9/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6389 - accuracy: 0.6444 - val_loss: 0.5918 - val_accuracy: 0.7111\n",
      "Epoch 10/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6245 - accuracy: 0.6333 - val_loss: 0.6342 - val_accuracy: 0.6889\n",
      "Epoch 11/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6140 - accuracy: 0.7250 - val_loss: 0.6114 - val_accuracy: 0.6778\n",
      "Epoch 12/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.5987 - accuracy: 0.6750 - val_loss: 0.5834 - val_accuracy: 0.7222\n",
      "Epoch 13/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.6006 - accuracy: 0.6778 - val_loss: 0.6043 - val_accuracy: 0.6889\n",
      "Epoch 14/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.5776 - accuracy: 0.7000 - val_loss: 0.6110 - val_accuracy: 0.6778\n",
      "Epoch 15/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.5646 - accuracy: 0.7139 - val_loss: 0.6052 - val_accuracy: 0.6778\n",
      "Epoch 16/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.5586 - accuracy: 0.6972 - val_loss: 0.6458 - val_accuracy: 0.6333\n",
      "Epoch 17/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.5546 - accuracy: 0.7306 - val_loss: 0.5797 - val_accuracy: 0.7000\n",
      "Epoch 18/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.5434 - accuracy: 0.7139 - val_loss: 0.6400 - val_accuracy: 0.6889\n",
      "Epoch 19/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.5380 - accuracy: 0.7444 - val_loss: 0.6241 - val_accuracy: 0.6889\n",
      "Epoch 20/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.5203 - accuracy: 0.7417 - val_loss: 0.5704 - val_accuracy: 0.7111\n",
      "Epoch 21/50\n",
      "360/360 [==============================] - 8s 24ms/sample - loss: 0.5136 - accuracy: 0.7444 - val_loss: 0.6080 - val_accuracy: 0.7222\n",
      "Epoch 22/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.4944 - accuracy: 0.7639 - val_loss: 0.5636 - val_accuracy: 0.7111\n",
      "Epoch 23/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.4963 - accuracy: 0.7472 - val_loss: 0.5847 - val_accuracy: 0.7556\n",
      "Epoch 24/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.4757 - accuracy: 0.7583 - val_loss: 0.5559 - val_accuracy: 0.7333\n",
      "Epoch 25/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.4677 - accuracy: 0.7806 - val_loss: 0.5700 - val_accuracy: 0.7444\n",
      "Epoch 26/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.4456 - accuracy: 0.7750 - val_loss: 0.5971 - val_accuracy: 0.7778\n",
      "Epoch 27/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.4403 - accuracy: 0.7917 - val_loss: 0.5590 - val_accuracy: 0.7222\n",
      "Epoch 28/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.4394 - accuracy: 0.8194 - val_loss: 0.5658 - val_accuracy: 0.7444\n",
      "Epoch 29/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.4085 - accuracy: 0.8083 - val_loss: 0.5673 - val_accuracy: 0.7667\n",
      "Epoch 30/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.3904 - accuracy: 0.8278 - val_loss: 0.5700 - val_accuracy: 0.7222\n",
      "Epoch 31/50\n",
      "360/360 [==============================] - 9s 26ms/sample - loss: 0.4048 - accuracy: 0.8278 - val_loss: 0.6247 - val_accuracy: 0.7222\n",
      "Epoch 32/50\n",
      "360/360 [==============================] - 9s 25ms/sample - loss: 0.3835 - accuracy: 0.8083 - val_loss: 0.5714 - val_accuracy: 0.7778\n",
      "Epoch 33/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.3590 - accuracy: 0.8556 - val_loss: 0.5778 - val_accuracy: 0.7778\n",
      "Epoch 34/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.3438 - accuracy: 0.8611 - val_loss: 0.6168 - val_accuracy: 0.7111\n",
      "Epoch 35/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.3215 - accuracy: 0.8750 - val_loss: 0.5804 - val_accuracy: 0.7222\n",
      "Epoch 36/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.3156 - accuracy: 0.8750 - val_loss: 0.8955 - val_accuracy: 0.6000\n",
      "Epoch 37/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.3604 - accuracy: 0.8222 - val_loss: 0.5822 - val_accuracy: 0.7222\n",
      "Epoch 38/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.3175 - accuracy: 0.8722 - val_loss: 0.6215 - val_accuracy: 0.7222\n",
      "Epoch 39/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.3213 - accuracy: 0.8611 - val_loss: 0.5939 - val_accuracy: 0.7000\n",
      "Epoch 40/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.2837 - accuracy: 0.9167 - val_loss: 0.6126 - val_accuracy: 0.7222\n",
      "Epoch 41/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.2861 - accuracy: 0.8889 - val_loss: 0.8576 - val_accuracy: 0.6444\n",
      "Epoch 42/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.2780 - accuracy: 0.8694 - val_loss: 0.6924 - val_accuracy: 0.7333\n",
      "Epoch 43/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.2585 - accuracy: 0.9028 - val_loss: 0.7850 - val_accuracy: 0.6444\n",
      "Epoch 44/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.2405 - accuracy: 0.9000 - val_loss: 0.8563 - val_accuracy: 0.6333\n",
      "Epoch 45/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.2369 - accuracy: 0.9222 - val_loss: 0.6497 - val_accuracy: 0.6778\n",
      "Epoch 46/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.2479 - accuracy: 0.9222 - val_loss: 0.6629 - val_accuracy: 0.6778\n",
      "Epoch 47/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.2281 - accuracy: 0.9389 - val_loss: 0.8321 - val_accuracy: 0.6222\n",
      "Epoch 48/50\n",
      "360/360 [==============================] - 8s 23ms/sample - loss: 0.2233 - accuracy: 0.9167 - val_loss: 0.8693 - val_accuracy: 0.6222\n",
      "Epoch 49/50\n",
      "360/360 [==============================] - 9s 24ms/sample - loss: 0.1905 - accuracy: 0.9417 - val_loss: 0.8356 - val_accuracy: 0.6556\n",
      "Epoch 50/50\n",
      "360/360 [==============================] - 8s 24ms/sample - loss: 0.1982 - accuracy: 0.9250 - val_loss: 1.0968 - val_accuracy: 0.6222\n",
      "Test loss: 0.9736046961375645 / Test accuracy: 0.6714285612106323\n"
     ]
    }
   ],
   "source": [
    "# Fraction classifier\n",
    "keras.backend.clear_session()\n",
    "\n",
    "input_train,input_test,target_train,target_test,input_shape = kares_data(text_list,img_group_list[1],300,300,2)\n",
    "\n",
    "model_f = kares_setup(input_train,input_test,target_train,target_test,input_shape, 300, 300, 250, 50, 2 , 0.2 ,1)\n",
    "\n",
    "model_f.save('model_f_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
