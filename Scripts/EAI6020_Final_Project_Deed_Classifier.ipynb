{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier for Deed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Packages requirement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opencv-python        4.1.2.30   \n",
    "six                  1.13.0             \n",
    "slim                 0.1                \n",
    "tensorboard          2.0.2              \n",
    "tensorflow           2.0.0              \n",
    "tensorflow-estimator 2.0.1              \n",
    "tensorflow-gpu       2.0.0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYTHONPATH='~/models-master/research/slim'\n"
     ]
    }
   ],
   "source": [
    "#set PATH=C\n",
    "#echo %PATH% \n",
    "\n",
    "# here is NETS package pwd\n",
    "#PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "%env PYTHONPATH =  '~/models-master/research/slim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_show_old(img,name):\n",
    "    cv2.namedWindow(name, cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow(name, img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def img_show(img, code=cv2.COLOR_BGR2RGB):\n",
    "    cv_rgb = cv2.cvtColor(img, code)\n",
    "    fig, ax = plt.subplots(figsize=(16, 10))\n",
    "    ax.imshow(cv_rgb)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_arr(img,x,y):\n",
    "    img = Image.fromarray(cv2.cvtColor(img,cv2.COLOR_BGR2RGB)).convert('L')\n",
    "    #img.show()  \n",
    "    #img = Image.open('iCard_021873_1_Daba_Ayehush_H-Copy1.jpg').convert('L')\n",
    "    if img.size[0] != x or img.size[1] != y:\n",
    "        img = img.resize((x, y))\n",
    "\n",
    "    arr = []\n",
    "\n",
    "    for i in range(y):\n",
    "        for j in range(x):\n",
    "            # mnist 里的颜色是0代表白色（背景），1.0代表黑色\n",
    "            #print(img.getpixel((j, i)))\n",
    "            pixel = 1.0 - float(img.getpixel((j, i)))/255.0\n",
    "            # pixel = 255.0 - float(img.getpixel((j, i))) # 如果是0-255的颜色值\n",
    "            arr.append(pixel)\n",
    "            \n",
    "    return arr\n",
    "\n",
    "#img_to_arr(img_list[0],300,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(520, 250000)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_path = sorted(os.listdir('All_Data/'))\n",
    "#Returns a list of all folders with participant numbers\n",
    "img_list =[]\n",
    "for path in img_path:\n",
    "    img  = cv2.imread('All_Data/' + path) \n",
    "    img_list.append(img)\n",
    "    \n",
    "text_list = []    \n",
    "for img in img_list:\n",
    "    text_list.append(img_to_arr(img,500,500))\n",
    "    \n",
    "np.array(text_list).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data = pd.read_csv('label_data.csv')\n",
    "\n",
    "img_group_list = []\n",
    "\n",
    "img_group = []\n",
    "for i in np.array(img_data.iloc[:,[1]]).tolist():\n",
    "    if(i == [\"N\"]):\n",
    "        img_group.append([0])\n",
    "    elif(i == [\"Y\"]):\n",
    "        img_group.append([1])\n",
    "img_group_list.append(img_group)  \n",
    "\n",
    "img_group = []\n",
    "for i in np.array(img_data.iloc[:,[2]]).tolist():\n",
    "    if(i == [\"N\"]):\n",
    "        img_group.append([0])\n",
    "    elif(i == [\"Y\"]):\n",
    "        img_group.append([1])\n",
    "img_group_list.append(img_group)   \n",
    "\n",
    "\n",
    "len(img_group_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(450, 500, 500, 1)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_train = np.array(text_list)\n",
    "input_train = input_train.reshape(input_train.shape[0], 500, 500, 1)\n",
    "input_train.shape\n",
    "\n",
    "input_train[0:450].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "  Keras CNN\n",
    "'''\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "# Model configuration\n",
    "img_width, img_height = 500, 500\n",
    "batch_size = 250\n",
    "no_epochs = 100\n",
    "no_classes = 3\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "\n",
    "def kares_data(input_data,input_target,img_width,img_height,no_classes):\n",
    "        # Load dataset\n",
    "    input_data = np.array(input_data)\n",
    "    input_train = input_data[0:450]\n",
    "    target_train = input_target[0:450]\n",
    "    \n",
    "    input_test = input_data[450:520]\n",
    "    target_test = input_target[450:520]\n",
    "    \n",
    "    # Reshape data based on channels first / channels last strategy.\n",
    "    # This is dependent on whether you use TF, Theano or CNTK as backend.\n",
    "    # Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        input_train = input_train.reshape(input_train.shape[0], 1, img_width, img_height)\n",
    "        input_test = input_test.reshape(input_test.shape[0], 1, img_width, img_height)\n",
    "        input_shape = (1, img_width, img_height)\n",
    "    else:\n",
    "        input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\n",
    "        input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\n",
    "        input_shape = (img_width, img_height, 1)\n",
    "    \n",
    "    # Parse numbers as floats\n",
    "    input_train = input_train.astype('float32')\n",
    "    input_test = input_test.astype('float32')\n",
    "    \n",
    "    # Normalize data\n",
    "    input_train = input_train / 255\n",
    "    input_test = input_test / 255\n",
    "    \n",
    "    # Convert target vectors to categorical targets\n",
    "    target_train = keras.utils.to_categorical(target_train, no_classes)\n",
    "    target_test = keras.utils.to_categorical(target_test, no_classes)\n",
    "    \n",
    "    return input_train,input_test,target_train,target_test,input_shape\n",
    "    \n",
    "def kares_setup(input_train,input_test,target_train,target_test,input_shape,img_width, img_height,batch_size,no_epochs,no_classes,validation_split,verbosity):\n",
    "    \n",
    "    # Create the model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(6, kernel_size=(5, 5), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(10, kernel_size=(5, 5), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(no_classes, activation='softmax'))\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "    # Fit data to model\n",
    "    model.fit(input_train, target_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=no_epochs,\n",
    "          verbose=verbosity,\n",
    "          validation_split=validation_split)\n",
    "\n",
    "    # Generate generalization metrics\n",
    "    score = model.evaluate(input_test, target_test, verbose=0)\n",
    "    print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360 samples, validate on 90 samples\n",
      "Epoch 1/50\n",
      "360/360 [==============================] - 31s 86ms/sample - loss: 0.6698 - accuracy: 0.5472 - val_loss: 1.2177 - val_accuracy: 0.7333\n",
      "Epoch 2/50\n",
      "360/360 [==============================] - 25s 70ms/sample - loss: 1.3616 - accuracy: 0.6278 - val_loss: 0.7097 - val_accuracy: 0.2667\n",
      "Epoch 3/50\n",
      "360/360 [==============================] - 27s 74ms/sample - loss: 0.7003 - accuracy: 0.3722 - val_loss: 0.6941 - val_accuracy: 0.2667\n",
      "Epoch 4/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6934 - accuracy: 0.4444 - val_loss: 0.6926 - val_accuracy: 0.7333\n",
      "Epoch 5/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6928 - accuracy: 0.6278 - val_loss: 0.6917 - val_accuracy: 0.7333\n",
      "Epoch 6/50\n",
      "360/360 [==============================] - 26s 71ms/sample - loss: 0.6923 - accuracy: 0.6278 - val_loss: 0.6905 - val_accuracy: 0.7333\n",
      "Epoch 7/50\n",
      "360/360 [==============================] - 25s 70ms/sample - loss: 0.6916 - accuracy: 0.6278 - val_loss: 0.6891 - val_accuracy: 0.7333\n",
      "Epoch 8/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6908 - accuracy: 0.6278 - val_loss: 0.6876 - val_accuracy: 0.7333\n",
      "Epoch 9/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6900 - accuracy: 0.6278 - val_loss: 0.6859 - val_accuracy: 0.7333\n",
      "Epoch 10/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6891 - accuracy: 0.6278 - val_loss: 0.6841 - val_accuracy: 0.7333\n",
      "Epoch 11/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6881 - accuracy: 0.6278 - val_loss: 0.6823 - val_accuracy: 0.7333\n",
      "Epoch 12/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6871 - accuracy: 0.6278 - val_loss: 0.6804 - val_accuracy: 0.7333\n",
      "Epoch 13/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6862 - accuracy: 0.6278 - val_loss: 0.6784 - val_accuracy: 0.7333\n",
      "Epoch 14/50\n",
      "360/360 [==============================] - 25s 71ms/sample - loss: 0.6852 - accuracy: 0.6278 - val_loss: 0.6765 - val_accuracy: 0.7333\n",
      "Epoch 15/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6842 - accuracy: 0.6278 - val_loss: 0.6745 - val_accuracy: 0.7333\n",
      "Epoch 16/50\n",
      "360/360 [==============================] - 26s 72ms/sample - loss: 0.6832 - accuracy: 0.6278 - val_loss: 0.6726 - val_accuracy: 0.7333\n",
      "Epoch 17/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6821 - accuracy: 0.6278 - val_loss: 0.6706 - val_accuracy: 0.7333\n",
      "Epoch 18/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6812 - accuracy: 0.6278 - val_loss: 0.6686 - val_accuracy: 0.7333\n",
      "Epoch 19/50\n",
      "360/360 [==============================] - 25s 70ms/sample - loss: 0.6803 - accuracy: 0.6278 - val_loss: 0.6666 - val_accuracy: 0.7333\n",
      "Epoch 20/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6794 - accuracy: 0.6278 - val_loss: 0.6646 - val_accuracy: 0.7333\n",
      "Epoch 21/50\n",
      "360/360 [==============================] - 25s 70ms/sample - loss: 0.6784 - accuracy: 0.6278 - val_loss: 0.6628 - val_accuracy: 0.7333\n",
      "Epoch 22/50\n",
      "360/360 [==============================] - 25s 70ms/sample - loss: 0.6775 - accuracy: 0.6278 - val_loss: 0.6609 - val_accuracy: 0.7333\n",
      "Epoch 23/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6767 - accuracy: 0.6278 - val_loss: 0.6590 - val_accuracy: 0.7333\n",
      "Epoch 24/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6758 - accuracy: 0.6278 - val_loss: 0.6572 - val_accuracy: 0.7333\n",
      "Epoch 25/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6749 - accuracy: 0.6278 - val_loss: 0.6554 - val_accuracy: 0.7333\n",
      "Epoch 26/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6742 - accuracy: 0.6278 - val_loss: 0.6535 - val_accuracy: 0.7333\n",
      "Epoch 27/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6733 - accuracy: 0.6278 - val_loss: 0.6516 - val_accuracy: 0.7333\n",
      "Epoch 28/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6726 - accuracy: 0.6278 - val_loss: 0.6497 - val_accuracy: 0.7333\n",
      "Epoch 29/50\n",
      "360/360 [==============================] - 25s 71ms/sample - loss: 0.6716 - accuracy: 0.6278 - val_loss: 0.6479 - val_accuracy: 0.7333\n",
      "Epoch 30/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6710 - accuracy: 0.6278 - val_loss: 0.6461 - val_accuracy: 0.7333\n",
      "Epoch 31/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6703 - accuracy: 0.6278 - val_loss: 0.6443 - val_accuracy: 0.7333\n",
      "Epoch 32/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6696 - accuracy: 0.6278 - val_loss: 0.6426 - val_accuracy: 0.7333\n",
      "Epoch 33/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6689 - accuracy: 0.6278 - val_loss: 0.6409 - val_accuracy: 0.7333\n",
      "Epoch 34/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6682 - accuracy: 0.6278 - val_loss: 0.6392 - val_accuracy: 0.7333\n",
      "Epoch 35/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6677 - accuracy: 0.6278 - val_loss: 0.6375 - val_accuracy: 0.7333\n",
      "Epoch 36/50\n",
      "360/360 [==============================] - 26s 72ms/sample - loss: 0.6670 - accuracy: 0.6278 - val_loss: 0.6359 - val_accuracy: 0.7333\n",
      "Epoch 37/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6664 - accuracy: 0.6278 - val_loss: 0.6342 - val_accuracy: 0.7333\n",
      "Epoch 38/50\n",
      "360/360 [==============================] - 26s 72ms/sample - loss: 0.6658 - accuracy: 0.6278 - val_loss: 0.6325 - val_accuracy: 0.7333\n",
      "Epoch 39/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6653 - accuracy: 0.6278 - val_loss: 0.6308 - val_accuracy: 0.7333\n",
      "Epoch 40/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6649 - accuracy: 0.6278 - val_loss: 0.6292 - val_accuracy: 0.7333\n",
      "Epoch 41/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6644 - accuracy: 0.6278 - val_loss: 0.6277 - val_accuracy: 0.7333\n",
      "Epoch 42/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6638 - accuracy: 0.6278 - val_loss: 0.6263 - val_accuracy: 0.7333\n",
      "Epoch 43/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6635 - accuracy: 0.6278 - val_loss: 0.6249 - val_accuracy: 0.7333\n",
      "Epoch 44/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6632 - accuracy: 0.6278 - val_loss: 0.6234 - val_accuracy: 0.7333\n",
      "Epoch 45/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6627 - accuracy: 0.6278 - val_loss: 0.6221 - val_accuracy: 0.7333\n",
      "Epoch 46/50\n",
      "360/360 [==============================] - 25s 70ms/sample - loss: 0.6624 - accuracy: 0.6278 - val_loss: 0.6208 - val_accuracy: 0.7333\n",
      "Epoch 47/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6620 - accuracy: 0.6278 - val_loss: 0.6195 - val_accuracy: 0.7333\n",
      "Epoch 48/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6618 - accuracy: 0.6278 - val_loss: 0.6181 - val_accuracy: 0.7333\n",
      "Epoch 49/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6615 - accuracy: 0.6278 - val_loss: 0.6168 - val_accuracy: 0.7333\n",
      "Epoch 50/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6613 - accuracy: 0.6278 - val_loss: 0.6155 - val_accuracy: 0.7333\n",
      "Test loss: 0.5373307279178074 / Test accuracy: 0.9142857193946838\n"
     ]
    }
   ],
   "source": [
    "# Hand writing classifier\n",
    "keras.backend.clear_session()\n",
    "\n",
    "input_train,input_test,target_train,target_test,input_shape = kares_data(text_list,img_group_list[0],500,500,2)\n",
    "\n",
    "model_handwriting = kares_setup(input_train,input_test,target_train,target_test,input_shape, 500, 500, 250, 50, 2 , 0.2 ,1)\n",
    "\n",
    "model_handwriting.save('model_hw.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 360 samples, validate on 90 samples\n",
      "Epoch 1/50\n",
      "360/360 [==============================] - 33s 93ms/sample - loss: 0.6916 - accuracy: 0.4944 - val_loss: 0.8832 - val_accuracy: 0.3111\n",
      "Epoch 2/50\n",
      "360/360 [==============================] - 26s 73ms/sample - loss: 0.7637 - accuracy: 0.4444 - val_loss: 0.6810 - val_accuracy: 0.6889\n",
      "Epoch 3/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6909 - accuracy: 0.5333 - val_loss: 0.6841 - val_accuracy: 0.6889\n",
      "Epoch 4/50\n",
      "360/360 [==============================] - 27s 75ms/sample - loss: 0.6920 - accuracy: 0.5333 - val_loss: 0.6745 - val_accuracy: 0.6889\n",
      "Epoch 5/50\n",
      "360/360 [==============================] - 27s 74ms/sample - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6659 - val_accuracy: 0.6889\n",
      "Epoch 6/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6917 - accuracy: 0.5333 - val_loss: 0.6617 - val_accuracy: 0.6889\n",
      "Epoch 7/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6913 - accuracy: 0.5333 - val_loss: 0.6599 - val_accuracy: 0.6889\n",
      "Epoch 8/50\n",
      "360/360 [==============================] - 27s 74ms/sample - loss: 0.6913 - accuracy: 0.5333 - val_loss: 0.6751 - val_accuracy: 0.6889\n",
      "Epoch 9/50\n",
      "360/360 [==============================] - 27s 75ms/sample - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6820 - val_accuracy: 0.6889\n",
      "Epoch 10/50\n",
      "360/360 [==============================] - 28s 78ms/sample - loss: 0.6916 - accuracy: 0.5333 - val_loss: 0.6810 - val_accuracy: 0.6889\n",
      "Epoch 11/50\n",
      "360/360 [==============================] - 27s 74ms/sample - loss: 0.6914 - accuracy: 0.5333 - val_loss: 0.6711 - val_accuracy: 0.6889\n",
      "Epoch 12/50\n",
      "360/360 [==============================] - 27s 75ms/sample - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6559 - val_accuracy: 0.6889\n",
      "Epoch 13/50\n",
      "360/360 [==============================] - 27s 74ms/sample - loss: 0.6929 - accuracy: 0.5333 - val_loss: 0.6656 - val_accuracy: 0.6889\n",
      "Epoch 14/50\n",
      "360/360 [==============================] - 26s 73ms/sample - loss: 0.6920 - accuracy: 0.5333 - val_loss: 0.6671 - val_accuracy: 0.6889\n",
      "Epoch 15/50\n",
      "360/360 [==============================] - 28s 79ms/sample - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6582 - val_accuracy: 0.6889\n",
      "Epoch 16/50\n",
      "360/360 [==============================] - 28s 77ms/sample - loss: 0.6918 - accuracy: 0.5333 - val_loss: 0.6541 - val_accuracy: 0.6889\n",
      "Epoch 17/50\n",
      "360/360 [==============================] - 26s 72ms/sample - loss: 0.6923 - accuracy: 0.5333 - val_loss: 0.6648 - val_accuracy: 0.6889\n",
      "Epoch 18/50\n",
      "360/360 [==============================] - 26s 72ms/sample - loss: 0.6921 - accuracy: 0.5333 - val_loss: 0.6777 - val_accuracy: 0.6889\n",
      "Epoch 19/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6912 - accuracy: 0.5333 - val_loss: 0.6789 - val_accuracy: 0.6889\n",
      "Epoch 20/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6915 - accuracy: 0.5333 - val_loss: 0.6792 - val_accuracy: 0.6889\n",
      "Epoch 21/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6913 - accuracy: 0.5333 - val_loss: 0.6808 - val_accuracy: 0.6889\n",
      "Epoch 22/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6919 - accuracy: 0.5333 - val_loss: 0.6815 - val_accuracy: 0.6889\n",
      "Epoch 23/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6914 - accuracy: 0.5333 - val_loss: 0.6734 - val_accuracy: 0.6889\n",
      "Epoch 24/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6907 - accuracy: 0.5333 - val_loss: 0.6582 - val_accuracy: 0.6889\n",
      "Epoch 25/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6920 - accuracy: 0.5333 - val_loss: 0.6579 - val_accuracy: 0.6889\n",
      "Epoch 26/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6920 - accuracy: 0.5333 - val_loss: 0.6739 - val_accuracy: 0.6889\n",
      "Epoch 27/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6915 - accuracy: 0.5333 - val_loss: 0.6788 - val_accuracy: 0.6889\n",
      "Epoch 28/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6916 - accuracy: 0.5333 - val_loss: 0.6763 - val_accuracy: 0.6889\n",
      "Epoch 29/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6912 - accuracy: 0.5333 - val_loss: 0.6760 - val_accuracy: 0.6889\n",
      "Epoch 30/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6700 - val_accuracy: 0.6889\n",
      "Epoch 31/50\n",
      "360/360 [==============================] - 27s 74ms/sample - loss: 0.6909 - accuracy: 0.5333 - val_loss: 0.6615 - val_accuracy: 0.6889\n",
      "Epoch 32/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6913 - accuracy: 0.5333 - val_loss: 0.6643 - val_accuracy: 0.6889\n",
      "Epoch 33/50\n",
      "360/360 [==============================] - 26s 73ms/sample - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6724 - val_accuracy: 0.6889\n",
      "Epoch 34/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6916 - accuracy: 0.5333 - val_loss: 0.6784 - val_accuracy: 0.6889\n",
      "Epoch 35/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6912 - accuracy: 0.5333 - val_loss: 0.6730 - val_accuracy: 0.6889\n",
      "Epoch 36/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6908 - accuracy: 0.5333 - val_loss: 0.6627 - val_accuracy: 0.6889\n",
      "Epoch 37/50\n",
      "360/360 [==============================] - 25s 69ms/sample - loss: 0.6926 - accuracy: 0.5333 - val_loss: 0.6653 - val_accuracy: 0.6889\n",
      "Epoch 38/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6903 - accuracy: 0.5333 - val_loss: 0.6804 - val_accuracy: 0.6889\n",
      "Epoch 39/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6914 - accuracy: 0.5333 - val_loss: 0.6813 - val_accuracy: 0.6889\n",
      "Epoch 40/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6915 - accuracy: 0.5333 - val_loss: 0.6811 - val_accuracy: 0.6889\n",
      "Epoch 41/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6915 - accuracy: 0.5333 - val_loss: 0.6807 - val_accuracy: 0.6889\n",
      "Epoch 42/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6914 - accuracy: 0.5333 - val_loss: 0.6800 - val_accuracy: 0.6889\n",
      "Epoch 43/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6914 - accuracy: 0.5333 - val_loss: 0.6791 - val_accuracy: 0.6889\n",
      "Epoch 44/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6913 - accuracy: 0.5333 - val_loss: 0.6780 - val_accuracy: 0.6889\n",
      "Epoch 45/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6912 - accuracy: 0.5333 - val_loss: 0.6768 - val_accuracy: 0.6889\n",
      "Epoch 46/50\n",
      "360/360 [==============================] - 25s 70ms/sample - loss: 0.6911 - accuracy: 0.5333 - val_loss: 0.6758 - val_accuracy: 0.6889\n",
      "Epoch 47/50\n",
      "360/360 [==============================] - 25s 68ms/sample - loss: 0.6911 - accuracy: 0.5333 - val_loss: 0.6746 - val_accuracy: 0.6889\n",
      "Epoch 48/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6911 - accuracy: 0.5333 - val_loss: 0.6736 - val_accuracy: 0.6889\n",
      "Epoch 49/50\n",
      "360/360 [==============================] - 24s 67ms/sample - loss: 0.6910 - accuracy: 0.5333 - val_loss: 0.6728 - val_accuracy: 0.6889\n",
      "Epoch 50/50\n",
      "360/360 [==============================] - 24s 68ms/sample - loss: 0.6909 - accuracy: 0.5333 - val_loss: 0.6720 - val_accuracy: 0.6889\n",
      "Test loss: 0.6759024006979806 / Test accuracy: 0.6571428775787354\n"
     ]
    }
   ],
   "source": [
    "# Fraction classifier\n",
    "keras.backend.clear_session()\n",
    "\n",
    "input_train,input_test,target_train,target_test,input_shape = kares_data(text_list,img_group_list[1],500,500,2)\n",
    "\n",
    "model_f = kares_setup(input_train,input_test,target_train,target_test,input_shape, 500, 500, 250, 50, 2 , 0.2 ,1)\n",
    "\n",
    "model_f.save('model_f.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
